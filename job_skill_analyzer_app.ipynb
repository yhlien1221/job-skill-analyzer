{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9lBVTCb45Mce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de36d34-5988-4def-a3a9-85fc7ce9430c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit requests pandas trafilatura cloudscraper seaborn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import trafilatura\n",
        "import cloudscraper\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Load Colab Secrets ---\n",
        "def get_colab_secrets():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        id_val = userdata.get('ADZUNA_APP_ID')\n",
        "        key_val = userdata.get('ADZUNA_APP_KEY')\n",
        "        return id_val, key_val\n",
        "    except:\n",
        "        return \"\", \"\"\n",
        "\n",
        "secret_id, secret_key = get_colab_secrets()\n",
        "\n",
        "# --- 2. Configuration & UI Setup ---\n",
        "st.set_page_config(page_title=\"Job Skill Analyzer\", layout=\"wide\")\n",
        "st.title(\"üéØ Job Skill Analyzer: Data Science & Analytics\")\n",
        "st.markdown(\"Scan and analyze real-time job market requirements via Adzuna API.\")\n",
        "\n",
        "# --- 3. Sidebar Setup ---\n",
        "with st.sidebar:\n",
        "    st.header(\"üîë API Credentials\")\n",
        "    app_id = st.text_input(\"Adzuna APP ID\", value=secret_id if secret_id else \"\")\n",
        "    app_key = st.text_input(\"Adzuna APP KEY\", type=\"password\", value=secret_key if secret_key else \"\")\n",
        "\n",
        "    st.header(\"üîç Search Filters\")\n",
        "    country = st.selectbox(\"Country\", [\"us\", \"uk\", \"ca\", \"au\"], index=0)\n",
        "    city = st.text_input(\"City\", value=\"Baltimore\")\n",
        "    keywords_input = st.text_input(\"Keywords (Comma separated)\", \"data scientist, data analyst\")\n",
        "    keywords = [k.strip() for k in keywords_input.split(\",\")]\n",
        "\n",
        "    st.header(\"‚öôÔ∏è Settings\")\n",
        "    days_back = st.sidebar.slider(\"Days Back\", 7, 90, 60)\n",
        "    max_pages = st.sidebar.number_input(\"Max Pages\", 1, 10, 5) # Default to 5 to match original script\n",
        "\n",
        "# --- 4. Synchronized Title Filtering Rules ---\n",
        "TARGET_TITLE_KEYWORDS = [\"data scientist\", \"senior data scientist\", \"sr. data scientist\", \"lead data scientist\", \"principal data scientist\", \"staff data scientist\", \"data analyst\", \"senior data analyst\"]\n",
        "EXCLUDED_TITLE_KEYWORDS = [\"architect\", \"engineer\", \"manager\", \"consultant\", \"director\", \"vp\", \"vice president\", \"head of\", \"marketing\", \"sales\", \"product\"]\n",
        "\n",
        "# --- 5. Synchronized Skill Taxonomy ---\n",
        "SKILL_KEYWORDS = {\n",
        "    'Programming': ['Python', 'SQL', ' R ', 'SAS', 'Stata', 'Julia', 'C++', 'Java', 'Scala', 'Go', 'Bash', 'Shell'],\n",
        "    'Cloud & Big Data': ['AWS', 'Azure', 'GCP', 'Google Cloud', 'Snowflake', 'Databricks', 'Spark', 'Hadoop', 'Kafka', 'Redshift', 'BigQuery', 'Athena', 'Glue', 'Terraform', 'Airflow'],\n",
        "    'Databases': ['PostgreSQL', 'MySQL', 'MongoDB', 'NoSQL', 'SQL Server', 'Oracle', 'Cassandra'],\n",
        "    'ML & AI': ['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'NLP', 'Natural Language Processing', 'Computer Vision', 'Generative AI', 'GenAI', 'LLM', 'GPT', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "    'Stats & Research': ['Statistics', 'statisti', 'Biostatistics', 'Causal Inference', 'Epidemiology', 'Econometrics', 'Bayesian', 'Survival Analysis', 'Longitudinal', 'Time Series', 'A/B Testing', 'Experimental Design', 'Propensity Score', 'Clinical Trials', 'Regression', 'Hypothesis Testing', 'RCT', 'GIS', 'Geographic Information Systems', 'Spatial'],\n",
        "    'Visualization & BI': ['Tableau', 'Power BI', 'Looker', 'Qlik', 'Matplotlib', 'Seaborn', 'Plotly', 'Shiny', 'D3.js'],\n",
        "    'Engineering & DevOps': ['Git', 'GitHub', 'CI/CD', 'Docker', 'Kubernetes', 'MLOps', 'Agile', 'Scrum', 'DevOps'],\n",
        "    'Degree': ['PhD', 'Ph.D.', 'Doctorate', 'Master', 'M.S.', 'MSc', 'MPH', 'MBA', 'Bachelor']\n",
        "}\n",
        "\n",
        "def is_relevant_title(title):\n",
        "    if not title: return False\n",
        "    t = title.lower()\n",
        "    has_target = any(k in t for k in TARGET_TITLE_KEYWORDS)\n",
        "    has_excluded = any(k in t for k in EXCLUDED_TITLE_KEYWORDS)\n",
        "    return has_target and not has_excluded\n",
        "\n",
        "def find_skills_refined(text):\n",
        "    if pd.isna(text): return []\n",
        "    res = []\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        for k in kws:\n",
        "            pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip()\n",
        "                if val == 'Ph.D.': val = 'PhD'\n",
        "                if val == 'M.S.': val = 'Master'\n",
        "                res.append(val)\n",
        "    return list(set(res))\n",
        "\n",
        "def fetch_full_description(url):\n",
        "    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})\n",
        "    try:\n",
        "        # Retry logic for anti-403\n",
        "        for _ in range(2):\n",
        "            response = scraper.get(url, timeout=20)\n",
        "            if response.status_code == 200:\n",
        "                content = trafilatura.extract(response.text, include_tables=True)\n",
        "                if content and len(content) > 300:\n",
        "                    return content.strip()\n",
        "            time.sleep(1.5)\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "# --- 6. Execution Logic ---\n",
        "if st.sidebar.button(\"üöÄ Start Analysis\"):\n",
        "    if not app_id or not app_key:\n",
        "        st.error(\"Please provide Adzuna API Credentials.\")\n",
        "    else:\n",
        "        all_jobs = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "        base_url = f\"https://api.adzuna.com/v1/api/jobs/{country}/search\"\n",
        "\n",
        "        status_text = st.empty()\n",
        "        progress_bar = st.progress(0)\n",
        "        total_steps = len(keywords) * max_pages\n",
        "        current_step = 0\n",
        "\n",
        "        for kw in keywords:\n",
        "            for page in range(1, max_pages + 1):\n",
        "                current_step += 1\n",
        "                status_text.info(f\"Fetching: {kw} (Page {page}/{max_pages})...\")\n",
        "                progress_bar.progress(current_step / total_steps)\n",
        "\n",
        "                params = {\"app_id\": app_id, \"app_key\": app_key, \"what\": kw, \"where\": city, \"results_per_page\": 50, \"sort_by\": \"date\"}\n",
        "                try:\n",
        "                    r = requests.get(f\"{base_url}/{page}\", params=params)\n",
        "                    data = r.json().get(\"results\", [])\n",
        "                    if not data: break\n",
        "\n",
        "                    for job in data:\n",
        "                        created_dt = datetime.fromisoformat(job.get(\"created\").replace(\"Z\", \"\"))\n",
        "                        if created_dt < cutoff_date: continue\n",
        "\n",
        "                        title = job.get(\"title\")\n",
        "                        if not is_relevant_title(title): continue\n",
        "\n",
        "                        # Fetch full description - CRITICAL FOR SYNCING COUNTS\n",
        "                        st.write(f\"üîç Analyzing: {title[:40]}...\")\n",
        "                        full_desc = fetch_full_description(job.get(\"redirect_url\"))\n",
        "                        final_description = full_desc if full_desc else job.get(\"description\", \"\")\n",
        "\n",
        "                        all_jobs.append({\n",
        "                            \"Title\": title,\n",
        "                            \"Company\": job.get(\"company\", {}).get(\"display_name\"),\n",
        "                            \"Location\": job.get(\"location\", {}).get(\"display_name\"),\n",
        "                            \"Created\": job.get(\"created\"),\n",
        "                            \"Description\": final_description,\n",
        "                            \"URL\": job.get(\"redirect_url\")\n",
        "                        })\n",
        "                        time.sleep(1)\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"API Request failed: {e}\")\n",
        "                    break\n",
        "\n",
        "        if all_jobs:\n",
        "            # Sync Raw Total: Removed .drop_duplicates() to match script 2\n",
        "            df = pd.DataFrame(all_jobs)\n",
        "            df['found_skills_list'] = df['Description'].apply(find_skills_refined)\n",
        "            df_effective = df[df['found_skills_list'].map(len) > 0].copy()\n",
        "            n_sample = len(df_effective)\n",
        "\n",
        "            st.success(f\"Analysis Complete: Found {len(df)} relevant jobs (N={n_sample} with skill data).\")\n",
        "\n",
        "            st.subheader(\"üìã Filtered Job List\")\n",
        "            st.dataframe(df[['Title', 'Company', 'Location', 'Created']].head(20))\n",
        "\n",
        "            # --- Visualizations ---\n",
        "            st.subheader(\"üìä Skill Market Share\")\n",
        "            all_found = []\n",
        "            for _, row in df_effective.iterrows():\n",
        "                for s in row['found_skills_list']:\n",
        "                    for cat, ks in SKILL_KEYWORDS.items():\n",
        "                        norm_ks = [k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master') for k in ks]\n",
        "                        if s in norm_ks:\n",
        "                            all_found.append({'Category': cat, 'Skill': s})\n",
        "                            break\n",
        "\n",
        "            stats_df = pd.DataFrame(all_found)\n",
        "            categories = list(SKILL_KEYWORDS.keys())\n",
        "            cols_num = 2\n",
        "            rows_num = math.ceil(len(categories) / cols_num)\n",
        "\n",
        "            fig, axes = plt.subplots(rows_num, cols_num, figsize=(16, rows_num * 5))\n",
        "            axes = axes.flatten()\n",
        "            palette = sns.color_palette(\"viridis\", len(categories))\n",
        "\n",
        "            for i, cat in enumerate(categories):\n",
        "                cat_data = stats_df[stats_df['Category'] == cat]\n",
        "                ax = axes[i]\n",
        "                if not cat_data.empty:\n",
        "                    counts = cat_data['Skill'].value_counts().reset_index()\n",
        "                    counts.columns = ['Skill', 'Count']\n",
        "                    sns.barplot(data=counts, x='Count', y='Skill', ax=ax, color=palette[i])\n",
        "                    ax.set_title(f'Category: {cat}', fontsize=14, fontweight='bold', color='#2c3e50')\n",
        "                    ax.set_xlim(0, n_sample + 1)\n",
        "                    for p in ax.patches:\n",
        "                        w = int(p.get_width())\n",
        "                        ax.annotate(f'{w} ({(w/n_sample)*100:.1f}%)', (w, p.get_y() + p.get_height()/2),\n",
        "                                    xytext=(8, 0), textcoords='offset points', fontweight='bold')\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, 'No matches', ha='center', va='center', color='gray')\n",
        "                    ax.set_title(f'Category: {cat} (Empty)', fontsize=14, color='gray')\n",
        "\n",
        "            for j in range(i + 1, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "\n",
        "            # Synchronized Suptitle and Logic\n",
        "            plt.suptitle(f'Market Analysis: Data Scientist & Data Analyst in {city}\\n(Effective N = {n_sample} Jobs with Skills | Raw Total = {len(df)})',\n",
        "                         fontsize=22, fontweight='bold', y=0.98, color='#1a2a6c')\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.download_button(\"üì• Download Results (CSV)\", data=df.to_csv(index=False).encode('utf-8-sig'), file_name=\"jobs_market_analysis.csv\", mime=\"text/csv\")\n",
        "        else:\n",
        "            st.warning(\"No jobs matched your filters. Try increasing 'Days Back' or pages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsyygiXfgi_U",
        "outputId": "2e95e203-b192-41e3-b721-84f4ff930ac5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##testing\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import trafilatura\n",
        "import cloudscraper\n",
        "import re\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Load Colab Secrets ---\n",
        "def get_colab_secrets():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        id_val = userdata.get('ADZUNA_APP_ID')\n",
        "        key_val = userdata.get('ADZUNA_APP_KEY')\n",
        "        return id_val, key_val\n",
        "    except:\n",
        "        return \"\", \"\"\n",
        "\n",
        "secret_id, secret_key = get_colab_secrets()\n",
        "\n",
        "# --- 2. Configuration & UI Setup ---\n",
        "st.set_page_config(page_title=\"Job Skill Analyzer\", layout=\"wide\")\n",
        "st.title(\"üéØ Job Skill Analyzer: Data Science & Analytics\")\n",
        "st.markdown(\"Scan and analyze real-time job market requirements via Adzuna API.\")\n",
        "\n",
        "# --- 3. Sidebar Setup ---\n",
        "with st.sidebar:\n",
        "    st.header(\"üîë API Credentials\")\n",
        "    app_id = st.text_input(\"Adzuna APP ID\", value=secret_id if secret_id else \"\")\n",
        "    app_key = st.text_input(\"Adzuna APP KEY\", type=\"password\", value=secret_key if secret_key else \"\")\n",
        "\n",
        "    st.header(\"üîç Search Filters\")\n",
        "    country = st.selectbox(\"Country\", [\"us\", \"uk\", \"ca\", \"au\"], index=0)\n",
        "\n",
        "    city_options = [\"Baltimore\", \"Washington DC\", \"Ellicott City\", \"Columbia\", \"Towson\", \"Silver Spring\", \"Bethesda\", \"Rockville\"]\n",
        "    selected_cities = st.multiselect(\n",
        "        \"Select Cities\",\n",
        "        options=city_options,\n",
        "        default=[\"Baltimore\"],\n",
        "        help=\"Type and press Enter to add a custom city.\"\n",
        "    )\n",
        "\n",
        "    keywords_input = st.text_input(\"Keywords (Comma separated)\", \"data scientist, data analyst\")\n",
        "    keywords = [k.strip() for k in keywords_input.split(\",\")]\n",
        "\n",
        "    st.header(\"‚öôÔ∏è Settings\")\n",
        "    days_back = st.slider(\"Days Back\", 7, 120, 60)\n",
        "    max_pages = st.number_input(\"Max Pages per City/KW\", 1, 15, 5)\n",
        "\n",
        "# --- 4. Filtering & Skill Taxonomy ---\n",
        "TARGET_TITLE_KEYWORDS = [\"data scientist\", \"senior data scientist\", \"sr. data scientist\", \"lead data scientist\", \"principal data scientist\", \"staff data scientist\", \"data analyst\", \"senior data analyst\"]\n",
        "EXCLUDED_TITLE_KEYWORDS = [\"architect\", \"engineer\", \"manager\", \"consultant\", \"director\", \"vp\", \"vice president\", \"head of\", \"marketing\", \"sales\", \"product\"]\n",
        "\n",
        "SKILL_KEYWORDS = {\n",
        "    'Programming': ['Python', 'SQL', ' R ', 'SAS', 'Stata', 'Julia', 'C++', 'Java', 'Scala', 'Go', 'Bash', 'Shell'],\n",
        "    'Cloud & Big Data': ['AWS', 'Azure', 'GCP', 'Google Cloud', 'Snowflake', 'Databricks', 'Spark', 'Hadoop', 'Kafka', 'Redshift', 'BigQuery', 'Athena', 'Glue', 'Terraform', 'Airflow'],\n",
        "    'Databases': ['PostgreSQL', 'MySQL', 'MongoDB', 'NoSQL', 'SQL Server', 'Oracle', 'Cassandra'],\n",
        "    'ML & AI': ['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'NLP', 'Natural Language Processing', 'Computer Vision', 'Generative AI', 'GenAI', 'LLM', 'GPT', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "    'Stats & Research': ['Statistics', 'statisti', 'Biostatistics', 'Causal Inference', 'Epidemiology', 'Econometrics', 'Bayesian', 'Survival Analysis', 'Longitudinal', 'Time Series', 'A/B Testing', 'Experimental Design', 'Propensity Score', 'Clinical Trials', 'Regression', 'Hypothesis Testing', 'RCT', 'GIS', 'Geographic Information Systems', 'Spatial'],\n",
        "    'Visualization & BI': ['Tableau', 'Power BI', 'Looker', 'Qlik', 'Matplotlib', 'Seaborn', 'Plotly', 'Shiny', 'D3.js'],\n",
        "    'Engineering & DevOps': ['Git', 'GitHub', 'CI/CD', 'Docker', 'Kubernetes', 'MLOps', 'Agile', 'Scrum', 'DevOps'],\n",
        "    'Degree': ['PhD', 'Ph.D.', 'Doctorate', 'Master', 'M.S.', 'MSc', 'MPH', 'MBA', 'Bachelor']\n",
        "}\n",
        "\n",
        "def is_relevant_title(title):\n",
        "    if not title: return False\n",
        "    t = title.lower()\n",
        "    has_target = any(k in t for k in TARGET_TITLE_KEYWORDS)\n",
        "    has_excluded = any(k in t for k in EXCLUDED_TITLE_KEYWORDS)\n",
        "    return has_target and not has_excluded\n",
        "\n",
        "def find_skills_refined(text):\n",
        "    if pd.isna(text): return []\n",
        "    res = []\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        for k in kws:\n",
        "            pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip()\n",
        "                if val == 'Ph.D.': val = 'PhD'\n",
        "                if val == 'M.S.': val = 'Master'\n",
        "                res.append(val)\n",
        "    return list(set(res))\n",
        "\n",
        "def fetch_full_description(url):\n",
        "    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})\n",
        "    try:\n",
        "        for _ in range(2):\n",
        "            response = scraper.get(url, timeout=20)\n",
        "            if response.status_code == 200:\n",
        "                content = trafilatura.extract(response.text, include_tables=True)\n",
        "                if content and len(content) > 300:\n",
        "                    return content.strip()\n",
        "            time.sleep(1.5)\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "# --- 5. Main Execution Logic ---\n",
        "if st.sidebar.button(\"üöÄ Start Deep Analysis\"):\n",
        "    if not app_id or not app_key:\n",
        "        st.error(\"Please provide Adzuna API Credentials.\")\n",
        "    elif not selected_cities:\n",
        "        st.warning(\"Please select at least one city.\")\n",
        "    else:\n",
        "        st.warning(\"‚è≥ **Deep Scraping & Skill Extraction in Progress...** We are fetching full descriptions to ensure accurate skill detection. This takes time‚Äîthank you for your patience.\")\n",
        "\n",
        "        all_jobs = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "        base_url = f\"https://api.adzuna.com/v1/api/jobs/{country}/search\"\n",
        "\n",
        "        status_text = st.empty()\n",
        "        progress_bar = st.progress(0)\n",
        "\n",
        "        total_steps = len(selected_cities) * len(keywords) * max_pages\n",
        "        current_step = 0\n",
        "\n",
        "        for city in selected_cities:\n",
        "            for kw in keywords:\n",
        "                for page in range(1, max_pages + 1):\n",
        "                    current_step += 1\n",
        "                    status_text.info(f\"üìç Location: **{city}** | Keyword: **{kw}** (Page {page}/{max_pages})\")\n",
        "                    progress_bar.progress(current_step / total_steps)\n",
        "\n",
        "                    params = {\"app_id\": app_id, \"app_key\": app_key, \"what\": kw, \"where\": city, \"results_per_page\": 50, \"sort_by\": \"date\"}\n",
        "                    try:\n",
        "                        r = requests.get(f\"{base_url}/{page}\", params=params)\n",
        "                        data = r.json().get(\"results\", [])\n",
        "                        if not data: break\n",
        "\n",
        "                        for job in data:\n",
        "                            created_dt = datetime.fromisoformat(job.get(\"created\").replace(\"Z\", \"\"))\n",
        "                            if created_dt < cutoff_date: continue\n",
        "\n",
        "                            title = job.get(\"title\")\n",
        "                            if not is_relevant_title(title): continue\n",
        "\n",
        "                            # Deep Analysis for skill detection\n",
        "                            full_desc = fetch_full_description(job.get(\"redirect_url\"))\n",
        "                            final_description = full_desc if full_desc else job.get(\"description\", \"\")\n",
        "\n",
        "                            all_jobs.append({\n",
        "                                \"Title\": title,\n",
        "                                \"Company\": job.get(\"company\", {}).get(\"display_name\"),\n",
        "                                \"Location\": job.get(\"location\", {}).get(\"display_name\"),\n",
        "                                \"Created\": job.get(\"created\"),\n",
        "                                \"Description\": final_description,\n",
        "                                \"URL\": job.get(\"redirect_url\")\n",
        "                            })\n",
        "                            time.sleep(1)\n",
        "                    except Exception:\n",
        "                        break\n",
        "\n",
        "        if all_jobs:\n",
        "            df = pd.DataFrame(all_jobs) # Deduplication skipped to match original script\n",
        "            df['found_skills_list'] = df['Description'].apply(find_skills_refined)\n",
        "            df_effective = df[df['found_skills_list'].map(len) > 0].copy()\n",
        "            n_sample = len(df_effective)\n",
        "\n",
        "            st.success(f\"Analysis Complete! Locations: {len(selected_cities)} | Raw Total: {len(df)} | Jobs with Skills: {n_sample}\")\n",
        "\n",
        "            # --- Interactive Visualizations with Plotly ---\n",
        "            st.subheader(\"üìä Interactive Skill Analysis (Zoomable)\")\n",
        "            st.info(\"üí° **Tip:** Use your mouse to zoom/pan. Double-click to reset view. Hover over bars for details.\")\n",
        "\n",
        "            all_found = []\n",
        "            for _, row in df_effective.iterrows():\n",
        "                for s in row['found_skills_list']:\n",
        "                    for cat, ks in SKILL_KEYWORDS.items():\n",
        "                        norm_ks = [k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master') for k in ks]\n",
        "                        if s in norm_ks:\n",
        "                            all_found.append({'Category': cat, 'Skill': s})\n",
        "                            break\n",
        "\n",
        "            stats_df = pd.DataFrame(all_found)\n",
        "            categories = list(SKILL_KEYWORDS.keys())\n",
        "            cols_num = 2\n",
        "            rows_num = math.ceil(len(categories) / cols_num)\n",
        "\n",
        "            # Create Subplots\n",
        "            fig = make_subplots(\n",
        "                rows=rows_num, cols=cols_num,\n",
        "                subplot_titles=[f\"<b>{cat}</b>\" for cat in categories],\n",
        "                horizontal_spacing=0.15,\n",
        "                vertical_spacing=0.08\n",
        "            )\n",
        "\n",
        "            for i, cat in enumerate(categories):\n",
        "                row = (i // cols_num) + 1\n",
        "                col = (i % cols_num) + 1\n",
        "\n",
        "                cat_data = stats_df[stats_df['Category'] == cat]\n",
        "                if not cat_data.empty:\n",
        "                    counts = cat_data['Skill'].value_counts().sort_values(ascending=True)\n",
        "\n",
        "                    fig.add_trace(\n",
        "                        go.Bar(\n",
        "                            x=counts.values,\n",
        "                            y=counts.index,\n",
        "                            orientation='h',\n",
        "                            name=cat,\n",
        "                            marker=dict(color=px.colors.qualitative.Plotly[i % 10]),\n",
        "                            hovertemplate=f\"<b>%{{y}}</b><br>Mentions: %{{x}}<br>Market Share: %{{customdata:.1f}}%<extra></extra>\",\n",
        "                            customdata=(counts.values / n_sample) * 100,\n",
        "                            text=[f\"{v} ({(v/n_sample)*100:.1f}%)\" for v in counts.values],\n",
        "                            textposition='outside',\n",
        "                            textfont=dict(size=14, color='black')\n",
        "                        ),\n",
        "                        row=row, col=col\n",
        "                    )\n",
        "                else:\n",
        "                    fig.add_annotation(text=\"No data detected\", xref=\"x domain\", yref=\"y domain\", x=0.5, y=0.5, showarrow=False, row=row, col=col)\n",
        "\n",
        "            # Update Layout for Font Size and Style\n",
        "            fig.update_layout(\n",
        "                height=rows_num * 500,\n",
        "                width=None, # Auto-fit width\n",
        "                title_text=f\"<b>Market Analysis: Data Scientist & Data Analyst</b><br>Locations: {', '.join(selected_cities)}<br>(N = {n_sample} Jobs with Skills | Raw Total = {len(df)})\",\n",
        "                title_font=dict(size=28, color='#1a2a6c'),\n",
        "                title_x=0.5,\n",
        "                showlegend=False,\n",
        "                font=dict(size=14),\n",
        "                margin=dict(t=150, b=50, l=50, r=100)\n",
        "            )\n",
        "\n",
        "            # Update X-axes to have shared scale or enough room\n",
        "            fig.update_xaxes(title_text=\"Mentions\", tickfont=dict(size=12))\n",
        "            fig.update_yaxes(tickfont=dict(size=13))\n",
        "\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "            st.subheader(\"üìã Job Data Preview\")\n",
        "            st.dataframe(df[['Title', 'Company', 'Location', 'Created']].head(20))\n",
        "\n",
        "            st.download_button(\"üì• Download Full CSV\", data=df.to_csv(index=False).encode('utf-8-sig'), file_name=\"analysis_results.csv\", mime=\"text/csv\")\n",
        "        else:\n",
        "            st.warning(\"No jobs found. Try adjusting your filters.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9btvnMfu8uvB",
        "outputId": "f884a67f-7135-4cb0-e88f-d01ba6cd83a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ë®≠ÂÆö Token (ÊääÂºïËôüÂÖßÊèõÊàê‰Ω†ÁöÑ)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"API\")\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r21iREgeIdJi",
        "outputId": "6b62b767-d213-47c9-cb8f-234145a3d038"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://4f56a1f665ae.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Êñ∑ÈñãÊâÄÊúâ‰ΩúÁî®‰∏≠ÁöÑÈößÈÅì\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    for tunnel in tunnels:\n",
        "        print(f\"Disconnecting: {tunnel.public_url}\")\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 2. ÊÆ∫ÊéâÊâÄÊúâ ngrok Ëàá streamlit ÁöÑËÉåÊôØÈÄ≤Á®ã\n",
        "!pkill ngrok\n",
        "!pkill streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BrK17qeDVrX",
        "outputId": "2d13f7a0-e4e0-4c12-8069-eb71d699a562"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2026-02-03T19:49:21+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-ea2c82e6-a24f-4bb5-bb00-9bb3cff1898e acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2026-02-03T19:49:21+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-6f17a9d7-22f4-4e9f-bff4-93e6f37abd7a acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2026-02-03T19:49:21+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-5f9bce90-f97f-49e4-82d5-4053389b6c01 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disconnecting: https://8d5c7c31c169.ngrok-free.app\n",
            "Disconnecting: https://aca77c5cc45d.ngrok-free.app\n",
            "Disconnecting: https://905583b0899d.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}