{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lBVTCb45Mce",
        "outputId": "69700c45-d4fe-4b02-90ec-4fe8a0f4e8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.53.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting trafilatura\n",
            "  Using cached trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting cloudscraper\n",
            "  Using cached cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install streamlit requests pandas trafilatura cloudscraper seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import trafilatura\n",
        "import cloudscraper\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. é…ç½®èˆ‡ UI è¨­å®š ---\n",
        "st.set_page_config(page_title=\"Job Skill Analyzer\", layout=\"wide\")\n",
        "st.title(\"ğŸ¯ Job Skill Analyzer: Data Scientist & Analyst\")\n",
        "st.markdown(\"é€é Adzuna API æŠ“å–è·ç¼ºï¼Œä¸¦æ·±åº¦åˆ†ææŠ€èƒ½åˆ†ä½ˆã€‚\")\n",
        "\n",
        "# --- 2. å´é‚Šæ¬„è¨­å®š (å·²ä¿®å¾©é‡è¤‡å®šç¾©å•é¡Œ) ---\n",
        "with st.sidebar:\n",
        "    st.header(\"ğŸ”‘ API Credentials\")\n",
        "    app_id = st.text_input(\"Adzuna APP ID\", value=\"\", help=\"è«‹è¼¸å…¥æ‚¨çš„ Adzuna Application ID\")\n",
        "    app_key = st.text_input(\"Adzuna APP KEY\", type=\"password\", value=\"\", help=\"è«‹è¼¸å…¥æ‚¨çš„ Adzuna Application Key\")\n",
        "\n",
        "    st.header(\"ğŸ” Search Filters\")\n",
        "    country = st.selectbox(\"Country\", [\"us\", \"uk\", \"ca\", \"au\"], index=0)\n",
        "    city = st.text_input(\"City\", value=\"Baltimore\")\n",
        "    keywords_input = st.text_input(\"Keywords (é€—è™Ÿåˆ†éš”)\", \"data scientist, data analyst\")\n",
        "    keywords = [k.strip() for k in keywords_input.split(\",\")]\n",
        "\n",
        "    st.header(\"é …æ¬¡è¨­å®š\")\n",
        "    days_back = st.sidebar.slider(\"Days Back\", 7, 90, 60)\n",
        "    max_pages = st.sidebar.number_input(\"Max Pages\", 1, 10, 3)\n",
        "    results_per_page = 50\n",
        "\n",
        "# --- 3. æŠ€èƒ½æ¸…å–® ---\n",
        "SKILL_KEYWORDS = {\n",
        "    'Programming': ['Python', 'SQL', ' R ', 'SAS', 'Stata', 'Julia', 'C++', 'Java', 'Scala', 'Go', 'Bash', 'Shell'],\n",
        "    'Cloud & Big Data': ['AWS', 'Azure', 'GCP', 'Google Cloud', 'Snowflake', 'Databricks', 'Spark', 'Hadoop', 'Kafka', 'Redshift', 'BigQuery', 'Athena', 'Glue', 'Terraform', 'Airflow'],\n",
        "    'Databases': ['PostgreSQL', 'MySQL', 'MongoDB', 'NoSQL', 'SQL Server', 'Oracle', 'Cassandra'],\n",
        "    'ML & AI': ['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'NLP', 'Natural Language Processing', 'Computer Vision', 'Generative AI', 'GenAI', 'LLM', 'GPT', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "    'Stats & Research': ['Statistics', 'Biostatistics', 'Causal Inference', 'Epidemiology', 'Econometrics', 'Bayesian', 'Survival Analysis', 'Longitudinal', 'Time Series', 'A/B Testing', 'Experimental Design', 'Propensity Score', 'Clinical Trials', 'Regression', 'Hypothesis Testing', 'RCT', 'GIS'],\n",
        "    'Visualization & BI': ['Tableau', 'Power BI', 'Looker', 'Qlik', 'Matplotlib', 'Seaborn', 'Plotly', 'Shiny', 'D3.js'],\n",
        "    'Engineering & DevOps': ['Git', 'GitHub', 'CI/CD', 'Docker', 'Kubernetes', 'MLOps', 'Agile', 'Scrum', 'DevOps'],\n",
        "    'Degree': ['PhD', 'Ph.D.', 'Doctorate', 'Master', 'M.S.', 'MSc', 'MPH', 'MBA', 'Bachelor']\n",
        "}\n",
        "\n",
        "# --- 4. åŠŸèƒ½å‡½æ•¸ ---\n",
        "def find_skills_refined(text):\n",
        "    if pd.isna(text): return []\n",
        "    res = []\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        for k in kws:\n",
        "            pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip()\n",
        "                if val == 'Ph.D.': val = 'PhD'\n",
        "                if val == 'M.S.': val = 'Master'\n",
        "                res.append(val)\n",
        "    return list(set(res))\n",
        "\n",
        "def fetch_full_description(url):\n",
        "    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})\n",
        "    try:\n",
        "        response = scraper.get(url, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            content = trafilatura.extract(response.text)\n",
        "            return content.strip() if content and len(content) > 300 else None\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "# --- 5. ä¸»ç¨‹å¼é‚è¼¯ ---\n",
        "if st.sidebar.button(\"ğŸš€ Start Fetching & Analysis\"):\n",
        "    if not app_id or not app_key:\n",
        "        st.error(\"âŒ è«‹è¼¸å…¥ Adzuna API ID èˆ‡ Keyï¼\")\n",
        "    else:\n",
        "        all_jobs = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "        base_url = f\"https://api.adzuna.com/v1/api/jobs/{country}/search\"\n",
        "\n",
        "        status_text = st.empty()\n",
        "        progress_bar = st.progress(0)\n",
        "\n",
        "        total_steps = len(keywords) * max_pages\n",
        "        step = 0\n",
        "\n",
        "        for kw in keywords:\n",
        "            for page in range(1, max_pages + 1):\n",
        "                step += 1\n",
        "                status_text.info(f\"ğŸ“¥ æ­£åœ¨æŠ“å–: {kw} (ç¬¬ {page}/{max_pages} é )...\")\n",
        "                progress_bar.progress(step / total_steps)\n",
        "\n",
        "                params = {\n",
        "                    \"app_id\": app_id,\n",
        "                    \"app_key\": app_key,\n",
        "                    \"what\": kw,\n",
        "                    \"where\": city,\n",
        "                    \"results_per_page\": results_per_page,\n",
        "                    \"sort_by\": \"date\"\n",
        "                }\n",
        "\n",
        "                try:\n",
        "                    r = requests.get(f\"{base_url}/{page}\", params=params)\n",
        "                    r.raise_for_status()\n",
        "                    data = r.json().get(\"results\", [])\n",
        "                    if not data: break\n",
        "\n",
        "                    for job in data:\n",
        "                        created_dt = datetime.fromisoformat(job.get(\"created\").replace(\"Z\", \"\"))\n",
        "                        if created_dt < cutoff_date: continue\n",
        "\n",
        "                        # éš¨æ©ŸæŠ“å–å®Œæ•´æè¿°ä»¥ç²å¾—æ›´ç²¾ç¢ºæŠ€èƒ½ (é¸æ“‡æ€§é–‹å•Ÿï¼Œæœƒå¢åŠ ç­‰å¾…æ™‚é–“)\n",
        "                        # full_desc = fetch_full_description(job.get(\"redirect_url\"))\n",
        "                        # final_desc = full_desc if full_desc else job.get(\"description\", \"\")\n",
        "                        final_desc = job.get(\"description\", \"\") # å…ˆç”¨ API æä¾›çš„ç°¡ä»‹åŠ é€Ÿæ¸¬è©¦\n",
        "\n",
        "                        all_jobs.append({\n",
        "                            \"title\": job.get(\"title\"),\n",
        "                            \"company\": job.get(\"company\", {}).get(\"display_name\"),\n",
        "                            \"created\": job.get(\"created\"),\n",
        "                            \"description\": final_desc,\n",
        "                            \"url\": job.get(\"redirect_url\")\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"âš ï¸ API è«‹æ±‚ä¸­æ–·: {e}\")\n",
        "                    break\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        if all_jobs:\n",
        "            df = pd.DataFrame(all_jobs).drop_duplicates(subset=['url'])\n",
        "            df['found_skills'] = df['description'].apply(find_skills_refined)\n",
        "            df_effective = df[df['found_skills'].map(len) > 0].copy()\n",
        "            n_sample = len(df_effective)\n",
        "\n",
        "            st.success(f\"âœ… å®Œæˆï¼åˆ†æäº† {len(df)} ç­†è·ç¼ºï¼Œå…¶ä¸­ {n_sample} ç­†åŒ…å«é—œéµæŠ€èƒ½ã€‚\")\n",
        "\n",
        "            # --- æ•¸æ“šé è¦½ ---\n",
        "            st.subheader(\"ğŸ“‹ Latest Jobs Found\")\n",
        "            st.dataframe(df[['title', 'company', 'created']].head(10))\n",
        "\n",
        "            # --- ç¹ªè£½ 8 å€‹åˆ†çµ„åœ–è¡¨ ---\n",
        "            st.subheader(\"ğŸ“Š Skill Distribution by Category\")\n",
        "\n",
        "            all_data = []\n",
        "            for _, row in df_effective.iterrows():\n",
        "                for skill in row['found_skills']:\n",
        "                    for cat, ks in SKILL_KEYWORDS.items():\n",
        "                        check_list = [k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master') for k in ks]\n",
        "                        if skill in check_list:\n",
        "                            all_data.append({'Category': cat, 'Skill': skill})\n",
        "                            break\n",
        "\n",
        "            full_stats = pd.DataFrame(all_data)\n",
        "            categories = list(SKILL_KEYWORDS.keys())\n",
        "            cols_n = 2\n",
        "            rows_n = math.ceil(len(categories) / cols_n)\n",
        "\n",
        "            fig, axes = plt.subplots(rows_n, cols_n, figsize=(16, rows_n * 5))\n",
        "            axes = axes.flatten()\n",
        "            palette = sns.color_palette(\"husl\", len(categories))\n",
        "\n",
        "            for i, cat in enumerate(categories):\n",
        "                cat_df = full_stats[full_stats['Category'] == cat]\n",
        "                if not cat_df.empty:\n",
        "                    counts = cat_df['Skill'].value_counts().reset_index()\n",
        "                    counts.columns = ['Skill', 'Count']\n",
        "                    sns.barplot(data=counts, x='Count', y='Skill', ax=axes[i], color=palette[i])\n",
        "                    axes[i].set_title(f'Category: {cat}', fontsize=14, fontweight='bold')\n",
        "                    axes[i].set_xlim(0, n_sample + 1)\n",
        "\n",
        "                    for p in axes[i].patches:\n",
        "                        w = int(p.get_width())\n",
        "                        axes[i].annotate(f'{w} ({(w/n_sample)*100:.1f}%)',\n",
        "                                         (w, p.get_y() + p.get_height()/2),\n",
        "                                         xytext=(5, 0), textcoords='offset points', fontsize=10)\n",
        "                else:\n",
        "                    axes[i].text(0.5, 0.5, 'No Data Found', ha='center', va='center', transform=axes[i].transAxes)\n",
        "                    axes[i].set_title(f'Category: {cat} (Empty)', fontsize=14)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # --- ä¸‹è¼‰ ---\n",
        "            csv = df.to_csv(index=False).encode('utf-8-sig')\n",
        "            st.download_button(\"ğŸ“¥ Download Analysis Result (CSV)\", data=csv, file_name=\"job_market_analysis.csv\", mime=\"text/csv\")\n",
        "        else:\n",
        "            st.warning(\"ğŸ§ æ²’æ‰¾åˆ°è·ç¼ºï¼Œè«‹å˜—è©¦å¢åŠ  Days Back æˆ–ä¿®æ”¹é—œéµå­—ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YVJEflW6e2P",
        "outputId": "43d98a19-6a9d-4e97-b665-e03f3b35c20d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run app.py --server.address=localhost &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501 & curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4o-eSl7EI_M",
        "outputId": "0775d4d0-89f7-4e4d-eb74-81959d353a4d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 709ms\n",
            "\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0K34.48.255.141\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://evil-humans-send.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. å®‰è£å¥—ä»¶\n",
        "!pip install pyngrok pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P-SqyxQ8xXZ",
        "outputId": "a15245b9-0f0d-4b90-d1cf-c2f3f2f7eb62"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. è¨­å®š Token (æŠŠå¼•è™Ÿå…§æ›æˆä½ çš„)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(ngrok_api)\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r21iREgeIdJi",
        "outputId": "20e51fe3-1024-4f6e-f512-d1e12f88fa70"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://598d7cb3c5f3.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}