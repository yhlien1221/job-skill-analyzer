{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9lBVTCb45Mce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e6bb5a-e96b-4597-f8e0-8f9a77ea0a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/9.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/9.1 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m8.8/9.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/132.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit requests pandas trafilatura cloudscraper seaborn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import trafilatura\n",
        "import cloudscraper\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- 1. Load Colab Secrets ---\n",
        "def get_colab_secrets():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        id_val = userdata.get('ADZUNA_APP_ID')\n",
        "        key_val = userdata.get('ADZUNA_APP_KEY')\n",
        "        return id_val, key_val\n",
        "    except:\n",
        "        return \"\", \"\"\n",
        "\n",
        "secret_id, secret_key = get_colab_secrets()\n",
        "\n",
        "# --- 2. UI Setup ---\n",
        "st.set_page_config(page_title=\"Job Skill Analyzer\", layout=\"wide\")\n",
        "st.title(\"ğŸ¯ Job Skill Analyzer: Data Science & Analytics\")\n",
        "\n",
        "# --- 3. Sidebar ---\n",
        "with st.sidebar:\n",
        "    st.header(\"ğŸ”‘ API Credentials\")\n",
        "    app_id = st.text_input(\"Adzuna APP ID\", value=secret_id if secret_id else \"\")\n",
        "    app_key = st.text_input(\"Adzuna APP KEY\", type=\"password\", value=secret_key if secret_key else \"\")\n",
        "\n",
        "    st.header(\"ğŸ” Filters\")\n",
        "    country = st.selectbox(\"Country\", [\"us\", \"uk\", \"ca\", \"au\"], index=0)\n",
        "    city_options = [\"Baltimore\", \"Washington DC\", \"Ellicott City\", \"Columbia\", \"Towson\", \"Silver Spring\"]\n",
        "    selected_cities = st.multiselect(\"Select Cities\", options=city_options, default=[\"Baltimore\"])\n",
        "\n",
        "    keywords_input = st.text_input(\"Keywords\", \"data scientist, data analyst\")\n",
        "    keywords = [k.strip() for k in keywords_input.split(\",\")]\n",
        "\n",
        "    st.header(\"âš™ï¸ Settings\")\n",
        "    days_back = st.slider(\"Days Back\", 7, 90, 60)\n",
        "    max_pages = st.number_input(\"Max Pages\", 1, 10, 5)\n",
        "\n",
        "# --- 4. Taxonomy ---\n",
        "SKILL_KEYWORDS = {\n",
        "    'Programming': ['Python', 'SQL', ' R ', 'SAS', 'Stata', 'Julia', 'C++', 'Java', 'Scala', 'Go', 'Bash', 'Shell'],\n",
        "    'Cloud & Big Data': ['AWS', 'Azure', 'GCP', 'Google Cloud', 'Snowflake', 'Databricks', 'Spark', 'Hadoop', 'Kafka', 'Redshift', 'BigQuery', 'Athena', 'Glue', 'Terraform', 'Airflow'],\n",
        "    'Databases': ['PostgreSQL', 'MySQL', 'MongoDB', 'NoSQL', 'SQL Server', 'Oracle', 'Cassandra'],\n",
        "    'ML & AI': ['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'NLP', 'Natural Language Processing', 'Computer Vision', 'Generative AI', 'GenAI', 'LLM', 'GPT', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost', 'LightGBM', 'CatBoost','AI'],\n",
        "    'Stats & Research': ['Statistics', 'statisti', 'Biostatistics', 'Causal Inference', 'Epidemiology', 'Econometrics', 'Bayesian', 'Survival Analysis', 'Longitudinal', 'Time Series', 'A/B Testing', 'Experimental Design', 'Propensity Score', 'Clinical Trials', 'Regression', 'Hypothesis Testing', 'RCT', 'GIS', 'Geographic Information Systems', 'Spatial'],\n",
        "    'Visualization & BI': ['Tableau', 'Power BI', 'Looker', 'Qlik', 'Matplotlib', 'Seaborn', 'Plotly', 'Shiny', 'D3.js', 'Insights'],\n",
        "    'Engineering & DevOps': [\n",
        "    'Git', 'GitHub', 'CI/CD', 'Docker', 'Kubernetes', 'MLOps', 'Agile', 'Scrum', 'DevOps',\n",
        "    'Ansible', 'Terraform', 'Infrastructure as Code', 'IaC', 'Prometheus', 'Grafana', 'ELK stack', 'Logging'],\n",
        "    'Degree': ['PhD', 'Ph.D.', 'Doctorate', 'Master', 'M.S.', 'MSc', 'MPH', 'MBA', 'Bachelor'],\n",
        "    'Experience': ['0-2 years', '3-5 years', '5-8 years', '8+ years'],\n",
        "    'Soft Skills': ['Leadership', 'Communication', 'Stakeholder', 'Management', 'Mentoring', 'Problem Solving', 'Presentation'],\n",
        "    'Business & Data Ops': ['ETL', 'Data Pipeline', 'Data Modeling', 'KPI', 'ROI', 'Strategy', 'A/B Testing', 'Product Analytics']\n",
        "\n",
        "} #soft skill (leadship, ), how many years, which industries, fuzzy matching, ETL, data analysis, dashboard, define kpi(business matrics)\n",
        "\n",
        "def is_relevant_title(title):\n",
        "    target = [\"data scientist\", \"senior data scientist\", \"sr. data scientist\", \"lead data scientist\", \"principal data scientist\", \"staff data scientist\", \"data analyst\", \"senior data analyst\"]\n",
        "    exclude = [\"architect\", \"engineer\", \"manager\", \"consultant\", \"director\", \"vp\", \"vice president\", \"head of\", \"marketing\", \"sales\", \"product\"]\n",
        "    t = title.lower()\n",
        "    return any(k in t for k in target) and not any(k in t for k in exclude)\n",
        "\n",
        "def find_skills_refined(text):\n",
        "    if pd.isna(text): return []\n",
        "    res = []\n",
        "\n",
        "    # 1. è‡ªå‹•æŠ“å–å¹´è³‡ä¸¦æ­¸é¡ (ç¶­æŒä½ ç›®å‰çš„è°æ˜é‚è¼¯)\n",
        "    exp_pattern = r'(\\d+)\\+?\\s*(?:years?|yrs?)\\b'\n",
        "    exp_matches = re.findall(exp_pattern, text, re.I)\n",
        "\n",
        "    if exp_matches:\n",
        "      years = [int(m) for m in exp_matches]\n",
        "      max_yr = max(years)\n",
        "      if max_yr >= 8: res.append('8+ years')\n",
        "      elif max_yr >= 5: res.append('5-8 years')\n",
        "      elif max_yr >= 3: res.append('3-5 years')\n",
        "      else: res.append('0-2 years')\n",
        "\n",
        "    # 2. é—œéµå­—æ¯”å°\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        # --- æ–°å¢é€™ä¸€è¡Œï¼šè·³é Experienceï¼Œå› ç‚ºä¸Šé¢å·²ç¶“è™•ç†éäº† ---\n",
        "        if cat == 'Experience': continue\n",
        "\n",
        "        for k in kws:\n",
        "            if k.strip() == 'R':\n",
        "                pattern = r'(?i)\\bR\\b'\n",
        "            else:\n",
        "                pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master')\n",
        "                res.append(val)\n",
        "\n",
        "    return list(set(res))\n",
        "\n",
        "    # --- [å„ªåŒ–] åŸæœ¬çš„é—œéµå­—æ¯”å° (åŠ ä¸Š R çš„ç²¾æº–åŒ¹é…) ---\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        for k in kws:\n",
        "            # é‡å° \" R \" åšç‰¹æ®Šè™•ç†ï¼Œé¿å…æŠ“åˆ°å–®å­—è£¡çš„ r\n",
        "            if k.strip() == 'R':\n",
        "                pattern = r'(?i)\\bR\\b'\n",
        "            else:\n",
        "                pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master')\n",
        "                res.append(val)\n",
        "\n",
        "    return list(set(res))\n",
        "\n",
        "def fetch_full_description(url):\n",
        "    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})\n",
        "    try:\n",
        "        response = scraper.get(url, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            content = trafilatura.extract(response.text)\n",
        "            return content.strip() if content and len(content) > 300 else None\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "# --- 5. Main Execution ---\n",
        "if st.sidebar.button(\"ğŸš€ Start Deep Analysis\"):\n",
        "    if not app_id or not app_key:\n",
        "        st.error(\"Missing Credentials.\")\n",
        "    else:\n",
        "        st.warning(\"â³ **Deep Scraping Active:** Fetching full descriptions for accuracy. This will take a few minutes...\")\n",
        "\n",
        "        all_jobs = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "\n",
        "        with st.status(\"Fetching Data...\", expanded=True) as status:\n",
        "            for city in selected_cities:\n",
        "                for kw in keywords:\n",
        "                    for page in range(1, max_pages + 1):\n",
        "                        params = {\"app_id\": app_id, \"app_key\": app_key, \"what\": kw, \"where\": city, \"results_per_page\": 50, \"sort_by\": \"date\"}\n",
        "                        try:\n",
        "                            r = requests.get(f\"https://api.adzuna.com/v1/api/jobs/{country}/search/{page}\", params=params)\n",
        "                            data = r.json().get(\"results\", [])\n",
        "                            if not data: break\n",
        "                            for job in data:\n",
        "                                if not is_relevant_title(job.get(\"title\")): continue\n",
        "                                full_desc = fetch_full_description(job.get(\"redirect_url\"))\n",
        "                                all_jobs.append({\n",
        "                                    \"Title\": job.get(\"title\"),\n",
        "                                    \"Company\": job.get(\"company\", {}).get(\"display_name\"),\n",
        "                                    \"Description\": full_desc if full_desc else job.get(\"description\", \"\"),\n",
        "                                    \"URL\": job.get(\"redirect_url\")\n",
        "                                })\n",
        "                                time.sleep(0.5)\n",
        "                        except: break\n",
        "            status.update(label=\"Scanning Complete!\", state=\"complete\", expanded=False)\n",
        "\n",
        "        if all_jobs:\n",
        "            df = pd.DataFrame(all_jobs)\n",
        "            df['skills'] = df['Description'].apply(find_skills_refined)\n",
        "            df_effective = df[df['skills'].map(len) > 0].copy()\n",
        "            n_sample = len(df_effective)\n",
        "\n",
        "            # --- å…¨æ–°åŠ å¼·ç‰ˆè¦–è¦ºåŒ– ---\n",
        "            st.subheader(\"ğŸ“Š Skill Analysis Results\")\n",
        "\n",
        "            all_found = []\n",
        "            for s_list in df_effective['skills']:\n",
        "                for s in s_list:\n",
        "                    for cat, ks in SKILL_KEYWORDS.items():\n",
        "                        if s in [k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master') for k in ks]:\n",
        "                            all_found.append({'Category': cat, 'Skill': s})\n",
        "\n",
        "            stats_df = pd.DataFrame(all_found)\n",
        "            cols = 2\n",
        "            rows = math.ceil(len(SKILL_KEYWORDS) / cols)\n",
        "\n",
        "            # --- é—œéµä¿®æ­£ï¼šå¼·åˆ¶æ”¾å¤§æ‰€æœ‰å­—é«” ---\n",
        "            # ç›´æ¥ä¿®æ”¹å…¨å±€åƒæ•¸\n",
        "            plt.rcParams.update({\n",
        "                'font.size': 22,\n",
        "                'axes.titlesize': 28,\n",
        "                'axes.labelsize': 20,\n",
        "                'ytick.labelsize': 20,\n",
        "                'xtick.labelsize': 18,\n",
        "                'font.weight': 'bold'\n",
        "            })\n",
        "\n",
        "            fig, axes = plt.subplots(rows, cols, figsize=(22, rows * 7))\n",
        "            axes = axes.flatten()\n",
        "            palette = sns.color_palette(\"viridis\", len(SKILL_KEYWORDS))\n",
        "\n",
        "            for i, cat in enumerate(SKILL_KEYWORDS.keys()):\n",
        "                ax = axes[i]\n",
        "                cat_data = stats_df[stats_df['Category'] == cat]\n",
        "                if not cat_data.empty:\n",
        "                    counts = cat_data['Skill'].value_counts().reset_index()\n",
        "                    counts.columns = ['Skill', 'Count']\n",
        "                    sns.barplot(data=counts, x='Count', y='Skill', ax=ax, color=palette[i])\n",
        "                    ax.set_title(f'{cat}', pad=25) # é¡åˆ¥æ¨™é¡Œ\n",
        "                    ax.set_xlabel('Mentions', labelpad=15)\n",
        "                    ax.set_ylabel('')\n",
        "                    ax.set_xlim(0, n_sample * 1.25)\n",
        "\n",
        "                    # æ•¸æ“šæ¨™ç±¤åŠ å¼·\n",
        "                    for p in ax.patches:\n",
        "                        w = int(p.get_width())\n",
        "                        ax.annotate(f'{w} ({(w/n_sample)*100:.1f}%)',\n",
        "                                    (w, p.get_y() + p.get_height()/2),\n",
        "                                    xytext=(15, 0), textcoords='offset points',\n",
        "                                    fontweight='bold', fontsize=18, va='center')\n",
        "                else:\n",
        "                    ax.set_title(f'{cat} (No Data)')\n",
        "\n",
        "            for j in range(i+1, len(axes)): fig.delaxes(axes[j])\n",
        "\n",
        "            # ç¸½æ¨™é¡ŒåŠ å¼·\n",
        "            plt.suptitle(f'Market Analysis: Data Science & Analytics\\nLocations: {\", \".join(selected_cities)}\\n(N = {n_sample} Jobs with Skills | Total = {len(df)})',\n",
        "                         fontsize=34, fontweight='bold', y=0.98, color='#1a2a6c')\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.94])\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.download_button(\"ğŸ“¥ Download Results (CSV)\", data=df.to_csv(index=False).encode('utf-8-sig'), file_name=\"job_analysis.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAL17GsdJeW8",
        "outputId": "1e220bff-7f09-4721-f49b-6af88b0a2c2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. è¨­å®š Token (æŠŠå¼•è™Ÿå…§æ›æˆä½ çš„)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"API\")\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r21iREgeIdJi",
        "outputId": "5bfbdd2a-4b2b-41e0-9613-9809df4e63d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://cdf6-136-107-31-104.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. æ–·é–‹æ‰€æœ‰ä½œç”¨ä¸­çš„éš§é“ #tes\n",
        "try:\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    for tunnel in tunnels:\n",
        "        print(f\"Disconnecting: {tunnel.public_url}\")\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# 2. æ®ºæ‰æ‰€æœ‰ ngrok èˆ‡ streamlit çš„èƒŒæ™¯é€²ç¨‹\n",
        "!pkill ngrok\n",
        "!pkill streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BrK17qeDVrX",
        "outputId": "f8ea1ec2-5da4-4518-e164-665a60c28ad6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2026-02-26T20:56:29+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-c147f85d-53ff-4ba6-9483-311eafbe2e10 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2026-02-26T20:56:29+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-e529ae1a-8ea9-4c30-bfc2-2d5392564fc5 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disconnecting: https://b8a2-136-107-31-104.ngrok-free.app\n",
            "Disconnecting: https://4d63-136-107-31-104.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}