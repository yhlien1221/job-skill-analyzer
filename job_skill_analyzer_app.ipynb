{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9lBVTCb45Mce"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit requests pandas trafilatura cloudscraper seaborn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import trafilatura\n",
        "import cloudscraper\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1. Load Colab Secrets ---\n",
        "def get_colab_secrets():\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        id_val = userdata.get('ADZUNA_APP_ID')\n",
        "        key_val = userdata.get('ADZUNA_APP_KEY')\n",
        "        return id_val, key_val\n",
        "    except:\n",
        "        return \"\", \"\"\n",
        "\n",
        "secret_id, secret_key = get_colab_secrets()\n",
        "\n",
        "# --- 2. Configuration & UI Setup ---\n",
        "st.set_page_config(page_title=\"Job Skill Analyzer\", layout=\"wide\")\n",
        "st.title(\"üéØ Job Skill Analyzer: Data Science & Analytics\")\n",
        "st.markdown(\"Scan and analyze real-time job market requirements via Adzuna API.\")\n",
        "\n",
        "# --- 3. Sidebar Setup ---\n",
        "with st.sidebar:\n",
        "    st.header(\"üîë API Credentials\")\n",
        "    app_id = st.text_input(\"Adzuna APP ID\", value=secret_id if secret_id else \"\")\n",
        "    app_key = st.text_input(\"Adzuna APP KEY\", type=\"password\", value=secret_key if secret_key else \"\")\n",
        "\n",
        "    st.header(\"üîç Search Filters\")\n",
        "    country = st.selectbox(\"Country\", [\"us\", \"uk\", \"ca\", \"au\"], index=0)\n",
        "    city = st.text_input(\"City\", value=\"Baltimore\")\n",
        "    keywords_input = st.text_input(\"Keywords (Comma separated)\", \"data scientist, data analyst\")\n",
        "    keywords = [k.strip() for k in keywords_input.split(\",\")]\n",
        "\n",
        "    st.header(\"‚öôÔ∏è Settings\")\n",
        "    days_back = st.sidebar.slider(\"Days Back\", 7, 90, 60)\n",
        "    max_pages = st.sidebar.number_input(\"Max Pages\", 1, 10, 5) # Default to 5 to match original script\n",
        "\n",
        "# --- 4. Synchronized Title Filtering Rules ---\n",
        "TARGET_TITLE_KEYWORDS = [\"data scientist\", \"senior data scientist\", \"sr. data scientist\", \"lead data scientist\", \"principal data scientist\", \"staff data scientist\", \"data analyst\", \"senior data analyst\"]\n",
        "EXCLUDED_TITLE_KEYWORDS = [\"architect\", \"engineer\", \"manager\", \"consultant\", \"director\", \"vp\", \"vice president\", \"head of\", \"marketing\", \"sales\", \"product\"]\n",
        "\n",
        "# --- 5. Synchronized Skill Taxonomy ---\n",
        "SKILL_KEYWORDS = {\n",
        "    'Programming': ['Python', 'SQL', ' R ', 'SAS', 'Stata', 'Julia', 'C++', 'Java', 'Scala', 'Go', 'Bash', 'Shell'],\n",
        "    'Cloud & Big Data': ['AWS', 'Azure', 'GCP', 'Google Cloud', 'Snowflake', 'Databricks', 'Spark', 'Hadoop', 'Kafka', 'Redshift', 'BigQuery', 'Athena', 'Glue', 'Terraform', 'Airflow'],\n",
        "    'Databases': ['PostgreSQL', 'MySQL', 'MongoDB', 'NoSQL', 'SQL Server', 'Oracle', 'Cassandra'],\n",
        "    'ML & AI': ['Machine Learning', 'Deep Learning', 'Reinforcement Learning', 'NLP', 'Natural Language Processing', 'Computer Vision', 'Generative AI', 'GenAI', 'LLM', 'GPT', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Keras', 'XGBoost', 'LightGBM', 'CatBoost'],\n",
        "    'Stats & Research': ['Statistics', 'statisti', 'Biostatistics', 'Causal Inference', 'Epidemiology', 'Econometrics', 'Bayesian', 'Survival Analysis', 'Longitudinal', 'Time Series', 'A/B Testing', 'Experimental Design', 'Propensity Score', 'Clinical Trials', 'Regression', 'Hypothesis Testing', 'RCT', 'GIS', 'Geographic Information Systems', 'Spatial'],\n",
        "    'Visualization & BI': ['Tableau', 'Power BI', 'Looker', 'Qlik', 'Matplotlib', 'Seaborn', 'Plotly', 'Shiny', 'D3.js'],\n",
        "    'Engineering & DevOps': ['Git', 'GitHub', 'CI/CD', 'Docker', 'Kubernetes', 'MLOps', 'Agile', 'Scrum', 'DevOps'],\n",
        "    'Degree': ['PhD', 'Ph.D.', 'Doctorate', 'Master', 'M.S.', 'MSc', 'MPH', 'MBA', 'Bachelor']\n",
        "}\n",
        "\n",
        "def is_relevant_title(title):\n",
        "    if not title: return False\n",
        "    t = title.lower()\n",
        "    has_target = any(k in t for k in TARGET_TITLE_KEYWORDS)\n",
        "    has_excluded = any(k in t for k in EXCLUDED_TITLE_KEYWORDS)\n",
        "    return has_target and not has_excluded\n",
        "\n",
        "def find_skills_refined(text):\n",
        "    if pd.isna(text): return []\n",
        "    res = []\n",
        "    for cat, kws in SKILL_KEYWORDS.items():\n",
        "        for k in kws:\n",
        "            pattern = r'\\b' + re.escape(k.strip()) + r'\\b'\n",
        "            if re.search(pattern, text, re.I):\n",
        "                val = k.strip()\n",
        "                if val == 'Ph.D.': val = 'PhD'\n",
        "                if val == 'M.S.': val = 'Master'\n",
        "                res.append(val)\n",
        "    return list(set(res))\n",
        "\n",
        "def fetch_full_description(url):\n",
        "    scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True})\n",
        "    try:\n",
        "        # Retry logic for anti-403\n",
        "        for _ in range(2):\n",
        "            response = scraper.get(url, timeout=20)\n",
        "            if response.status_code == 200:\n",
        "                content = trafilatura.extract(response.text, include_tables=True)\n",
        "                if content and len(content) > 300:\n",
        "                    return content.strip()\n",
        "            time.sleep(1.5)\n",
        "    except: pass\n",
        "    return None\n",
        "\n",
        "# --- 6. Execution Logic ---\n",
        "if st.sidebar.button(\"üöÄ Start Analysis\"):\n",
        "    if not app_id or not app_key:\n",
        "        st.error(\"Please provide Adzuna API Credentials.\")\n",
        "    else:\n",
        "        all_jobs = []\n",
        "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
        "        base_url = f\"https://api.adzuna.com/v1/api/jobs/{country}/search\"\n",
        "\n",
        "        status_text = st.empty()\n",
        "        progress_bar = st.progress(0)\n",
        "        total_steps = len(keywords) * max_pages\n",
        "        current_step = 0\n",
        "\n",
        "        for kw in keywords:\n",
        "            for page in range(1, max_pages + 1):\n",
        "                current_step += 1\n",
        "                status_text.info(f\"Fetching: {kw} (Page {page}/{max_pages})...\")\n",
        "                progress_bar.progress(current_step / total_steps)\n",
        "\n",
        "                params = {\"app_id\": app_id, \"app_key\": app_key, \"what\": kw, \"where\": city, \"results_per_page\": 50, \"sort_by\": \"date\"}\n",
        "                try:\n",
        "                    r = requests.get(f\"{base_url}/{page}\", params=params)\n",
        "                    data = r.json().get(\"results\", [])\n",
        "                    if not data: break\n",
        "\n",
        "                    for job in data:\n",
        "                        created_dt = datetime.fromisoformat(job.get(\"created\").replace(\"Z\", \"\"))\n",
        "                        if created_dt < cutoff_date: continue\n",
        "\n",
        "                        title = job.get(\"title\")\n",
        "                        if not is_relevant_title(title): continue\n",
        "\n",
        "                        # Fetch full description - CRITICAL FOR SYNCING COUNTS\n",
        "                        st.write(f\"üîç Analyzing: {title[:40]}...\")\n",
        "                        full_desc = fetch_full_description(job.get(\"redirect_url\"))\n",
        "                        final_description = full_desc if full_desc else job.get(\"description\", \"\")\n",
        "\n",
        "                        all_jobs.append({\n",
        "                            \"Title\": title,\n",
        "                            \"Company\": job.get(\"company\", {}).get(\"display_name\"),\n",
        "                            \"Location\": job.get(\"location\", {}).get(\"display_name\"),\n",
        "                            \"Created\": job.get(\"created\"),\n",
        "                            \"Description\": final_description,\n",
        "                            \"URL\": job.get(\"redirect_url\")\n",
        "                        })\n",
        "                        time.sleep(1)\n",
        "                except Exception as e:\n",
        "                    st.warning(f\"API Request failed: {e}\")\n",
        "                    break\n",
        "\n",
        "        if all_jobs:\n",
        "            # Sync Raw Total: Removed .drop_duplicates() to match script 2\n",
        "            df = pd.DataFrame(all_jobs)\n",
        "            df['found_skills_list'] = df['Description'].apply(find_skills_refined)\n",
        "            df_effective = df[df['found_skills_list'].map(len) > 0].copy()\n",
        "            n_sample = len(df_effective)\n",
        "\n",
        "            st.success(f\"Analysis Complete: Found {len(df)} relevant jobs (N={n_sample} with skill data).\")\n",
        "\n",
        "            st.subheader(\"üìã Filtered Job List\")\n",
        "            st.dataframe(df[['Title', 'Company', 'Location', 'Created']].head(20))\n",
        "\n",
        "            # --- Visualizations ---\n",
        "            st.subheader(\"üìä Skill Market Share\")\n",
        "            all_found = []\n",
        "            for _, row in df_effective.iterrows():\n",
        "                for s in row['found_skills_list']:\n",
        "                    for cat, ks in SKILL_KEYWORDS.items():\n",
        "                        norm_ks = [k.strip().replace('Ph.D.', 'PhD').replace('M.S.', 'Master') for k in ks]\n",
        "                        if s in norm_ks:\n",
        "                            all_found.append({'Category': cat, 'Skill': s})\n",
        "                            break\n",
        "\n",
        "            stats_df = pd.DataFrame(all_found)\n",
        "            categories = list(SKILL_KEYWORDS.keys())\n",
        "            cols_num = 2\n",
        "            rows_num = math.ceil(len(categories) / cols_num)\n",
        "\n",
        "            fig, axes = plt.subplots(rows_num, cols_num, figsize=(16, rows_num * 5))\n",
        "            axes = axes.flatten()\n",
        "            palette = sns.color_palette(\"viridis\", len(categories))\n",
        "\n",
        "            for i, cat in enumerate(categories):\n",
        "                cat_data = stats_df[stats_df['Category'] == cat]\n",
        "                ax = axes[i]\n",
        "                if not cat_data.empty:\n",
        "                    counts = cat_data['Skill'].value_counts().reset_index()\n",
        "                    counts.columns = ['Skill', 'Count']\n",
        "                    sns.barplot(data=counts, x='Count', y='Skill', ax=ax, color=palette[i])\n",
        "                    ax.set_title(f'Category: {cat}', fontsize=14, fontweight='bold', color='#2c3e50')\n",
        "                    ax.set_xlim(0, n_sample + 1)\n",
        "                    for p in ax.patches:\n",
        "                        w = int(p.get_width())\n",
        "                        ax.annotate(f'{w} ({(w/n_sample)*100:.1f}%)', (w, p.get_y() + p.get_height()/2),\n",
        "                                    xytext=(8, 0), textcoords='offset points', fontweight='bold')\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, 'No matches', ha='center', va='center', color='gray')\n",
        "                    ax.set_title(f'Category: {cat} (Empty)', fontsize=14, color='gray')\n",
        "\n",
        "            for j in range(i + 1, len(axes)):\n",
        "                fig.delaxes(axes[j])\n",
        "\n",
        "            # Synchronized Suptitle and Logic\n",
        "            plt.suptitle(f'Market Analysis: Data Scientist & Data Analyst in {city}\\n(Effective N = {n_sample} Jobs with Skills | Raw Total = {len(df)})',\n",
        "                         fontsize=22, fontweight='bold', y=0.98, color='#1a2a6c')\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            st.download_button(\"üì• Download Results (CSV)\", data=df.to_csv(index=False).encode('utf-8-sig'), file_name=\"jobs_market_analysis.csv\", mime=\"text/csv\")\n",
        "        else:\n",
        "            st.warning(\"No jobs matched your filters. Try increasing 'Days Back' or pages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsyygiXfgi_U",
        "outputId": "0ca39926-a9fc-4048-ea17-94e520b523d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Ë®≠ÂÆö Token (ÊääÂºïËôüÂÖßÊèõÊàê‰Ω†ÁöÑ)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2uJIzWRcCi2AQTsnJHBejqplPK6_5y9YJrZUm2MafCA2Nffym\")\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r21iREgeIdJi",
        "outputId": "e3584118-cf3c-451c-afba-6bfd8be3e5c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://39607e84ff94.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}